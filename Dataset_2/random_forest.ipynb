{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T21:47:27.098029Z",
     "start_time": "2025-12-02T21:47:27.070794Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"breast_cancer_data.csv\", index_col=False)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:47:36.176367Z",
     "start_time": "2025-12-02T21:47:36.119111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df.drop(\"id\", axis = 1, inplace = True) #uneccessary for classification.\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({'M': 1, 'B' : 0}) #one hot encoding the target variable.\n",
    "\n",
    "#splitting features columns from the target variable column.\n",
    "X = df.drop(\"diagnosis\", axis =1)\n",
    "Y = df[\"diagnosis\"]\n",
    "\n",
    "#making a transformer that scales all numerical values.\n",
    "numerical_cols = X.columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [(\"num\", StandardScaler(), numerical_cols)])\n",
    "\n",
    "partitions = [\n",
    "    (0.2, 0.8, \"20/80\"),\n",
    "    (0.5, 0.5, \"50/50\"),\n",
    "    (0.8, 0.2, \"80/20\"),\n",
    "]"
   ],
   "id": "d2d6bdd227f6b202",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcompose\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ColumnTransformer\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mid\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#uneccessary for classification.\u001B[39;00m\n\u001B[32m      5\u001B[39m df[\u001B[33m\"\u001B[39m\u001B[33mdiagnosis\u001B[39m\u001B[33m\"\u001B[39m] = df[\u001B[33m\"\u001B[39m\u001B[33mdiagnosis\u001B[39m\u001B[33m\"\u001B[39m].map({\u001B[33m'\u001B[39m\u001B[33mM\u001B[39m\u001B[33m'\u001B[39m: \u001B[32m1\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mB\u001B[39m\u001B[33m'\u001B[39m : \u001B[32m0\u001B[39m}) \u001B[38;5;66;03m#one hot encoding the target variable.\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m#splitting features columns from the target variable column.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/cogs_118a_proj/.venv/lib/python3.13/site-packages/pandas/core/frame.py:5603\u001B[39m, in \u001B[36mDataFrame.drop\u001B[39m\u001B[34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[39m\n\u001B[32m   5455\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdrop\u001B[39m(\n\u001B[32m   5456\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   5457\u001B[39m     labels: IndexLabel | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   5464\u001B[39m     errors: IgnoreRaise = \u001B[33m\"\u001B[39m\u001B[33mraise\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   5465\u001B[39m ) -> DataFrame | \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   5466\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   5467\u001B[39m \u001B[33;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[32m   5468\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   5601\u001B[39m \u001B[33;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[32m   5602\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m5603\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5604\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5605\u001B[39m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5606\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5607\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5608\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5609\u001B[39m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m=\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5610\u001B[39m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5611\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/cogs_118a_proj/.venv/lib/python3.13/site-packages/pandas/core/generic.py:4810\u001B[39m, in \u001B[36mNDFrame.drop\u001B[39m\u001B[34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[39m\n\u001B[32m   4808\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes.items():\n\u001B[32m   4809\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m4810\u001B[39m         obj = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4812\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[32m   4813\u001B[39m     \u001B[38;5;28mself\u001B[39m._update_inplace(obj)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/cogs_118a_proj/.venv/lib/python3.13/site-packages/pandas/core/generic.py:4852\u001B[39m, in \u001B[36mNDFrame._drop_axis\u001B[39m\u001B[34m(self, labels, axis, level, errors, only_slice)\u001B[39m\n\u001B[32m   4850\u001B[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001B[32m   4851\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m4852\u001B[39m         new_axis = \u001B[43maxis\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4853\u001B[39m     indexer = axis.get_indexer(new_axis)\n\u001B[32m   4855\u001B[39m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[32m   4856\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/cogs_118a_proj/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:7136\u001B[39m, in \u001B[36mIndex.drop\u001B[39m\u001B[34m(self, labels, errors)\u001B[39m\n\u001B[32m   7134\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mask.any():\n\u001B[32m   7135\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m errors != \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m7136\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels[mask].tolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not found in axis\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   7137\u001B[39m     indexer = indexer[~mask]\n\u001B[32m   7138\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.delete(indexer)\n",
      "\u001B[31mKeyError\u001B[39m: \"['id'] not found in axis\""
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:54:52.869678Z",
     "start_time": "2025-12-02T20:54:13.180311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#starting code for random forest classification.\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "results=[]\n",
    "for training_size, testing_size, partition_name in partitions:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Partition: {partition_name} (Train/Test)\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "\n",
    "    current_partition_results = []\n",
    "    for trial in range(3):\n",
    "        print(f\"Trial: {trial+1}\")\n",
    "\n",
    "        #splitting training and testing data and building a pipeline for processing data and classification.\n",
    "        X_training, X_testing, Y_training, Y_testing = train_test_split(X, Y, stratify = Y, test_size=testing_size, random_state=42+trial)\n",
    "        random_forest_pipeline = Pipeline([(\"preprocessor\", preprocessor),\n",
    "                                          (\"classifier\", RandomForestClassifier(random_state=42 + trial))])\n",
    "\n",
    "        #Hyperparameters of random forest classifier.\n",
    "        parameter_grid = {\n",
    "            'classifier__n_estimators': [50, 100, 200],  # Number of trees\n",
    "            'classifier__max_depth': [None, 5, 10, 15, 25, 30],  # Maximum depth of trees\n",
    "            'classifier__min_samples_split': [2, 5, 10],  # Minimum samples to split a node\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],  # Minimum samples at a leaf node\n",
    "        }\n",
    "\n",
    "        #tuning hyperparameters\n",
    "        grid_search = GridSearchCV(random_forest_pipeline, parameter_grid, n_jobs = -1, cv = 5, verbose = 0,scoring=\"accuracy\")\n",
    "        grid_search.fit(X_training, Y_training)\n",
    "        optimum_model = grid_search.best_estimator_\n",
    "\n",
    "        #getting predictions from the best model with optimum hyperparameters.\n",
    "        training_prediction = optimum_model.predict(X_training)\n",
    "        testing_prediction = optimum_model.predict(X_testing)\n",
    "\n",
    "        #calculating accuracies for training, cross validation and testing.\n",
    "        training_accuracy = accuracy_score(Y_training, training_prediction)\n",
    "        testing_accuracy = accuracy_score(Y_testing, testing_prediction)\n",
    "        print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "        print(f\"Training Accuracy: {training_accuracy:.4f}\")\n",
    "        print(f\"Testing Accuracy: {testing_accuracy:.4f}\")\n",
    "        print(f\"Cross validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "        trial_results = {\n",
    "            \"Partition name\": partition_name,\n",
    "            \"Trial number\": trial+1,\n",
    "            \"Training accuracy\": training_accuracy,\n",
    "            \"Testing accuracy\": testing_accuracy,\n",
    "            \"Cross validation accuracy\":grid_search.best_score_,\n",
    "            \"Best parameters\":grid_search.best_params_\n",
    "        }\n",
    "        current_partition_results.append(trial_results)\n",
    "\n",
    "    #calculating average accuracies\n",
    "    avg_training_accuracy = np.mean([r[\"Training accuracy\"] for r in current_partition_results])\n",
    "    avg_testing_accuracy = np.mean([r[\"Testing accuracy\"] for r in current_partition_results])\n",
    "    avg_cross_validation_accuracy = np.mean([r[\"Cross validation accuracy\"] for r in current_partition_results])\n",
    "\n",
    "    avg_accuracy_summary = {\n",
    "        \"Partition name\": partition_name,\n",
    "        \"Partition results\": current_partition_results,\n",
    "        \"Average training accuracy\": avg_training_accuracy,\n",
    "        \"Average testing accuracy\": avg_testing_accuracy,\n",
    "        \"Average cross validation accuracy\": avg_cross_validation_accuracy,\n",
    "    }\n",
    "    results.append(avg_accuracy_summary)"
   ],
   "id": "25b758477bc3c689",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Partition: 20/80 (Train/Test)\n",
      "\n",
      "==================================================\n",
      "Trial: 1\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9605\n",
      "Cross validation accuracy: 0.9123\n",
      "Trial: 2\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9364\n",
      "Cross validation accuracy: 0.9826\n",
      "Trial: 3\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 50}\n",
      "Training Accuracy: 0.9912\n",
      "Testing Accuracy: 0.9386\n",
      "Cross validation accuracy: 0.9202\n",
      "\n",
      "==================================================\n",
      "Partition: 50/50 (Train/Test)\n",
      "\n",
      "==================================================\n",
      "Trial: 1\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 50}\n",
      "Training Accuracy: 0.9965\n",
      "Testing Accuracy: 0.9614\n",
      "Cross validation accuracy: 0.9542\n",
      "Trial: 2\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9684\n",
      "Cross validation accuracy: 0.9613\n",
      "Trial: 3\n",
      "Best Hyperparameters: {'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 50}\n",
      "Training Accuracy: 0.9965\n",
      "Testing Accuracy: 0.9474\n",
      "Cross validation accuracy: 0.9648\n",
      "\n",
      "==================================================\n",
      "Partition: 80/20 (Train/Test)\n",
      "\n",
      "==================================================\n",
      "Trial: 1\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9737\n",
      "Cross validation accuracy: 0.9670\n",
      "Trial: 2\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9561\n",
      "Cross validation accuracy: 0.9670\n",
      "Trial: 3\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100}\n",
      "Training Accuracy: 0.9956\n",
      "Testing Accuracy: 0.9386\n",
      "Cross validation accuracy: 0.9604\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:55:11.234987Z",
     "start_time": "2025-12-02T20:55:11.226171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#printing results of rf classifier and storing them into a csv file.\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final summary - random forest on Breast Cancer Dataset\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Partition: {result['Partition name']}\")\n",
    "    print(f\"  Avg Train Accuracy: {result[\"Average training accuracy\"]:.4f}\")\n",
    "    print(f\"  Avg CV Accuracy: {result['Average cross validation accuracy']:.4f}\")\n",
    "    print(f\"  Avg Test Accuracy: {result['Average testing accuracy']:.4f}\")\n",
    "    print()\n",
    "\n",
    "compiled_results = []\n",
    "for result in results:\n",
    "    for partition in result[\"Partition results\"]:\n",
    "        compiled_results.append(partition)\n",
    "\n",
    "random_forest_results_csv = pd.DataFrame(compiled_results)\n",
    "random_forest_results_csv.to_csv(\"random_forest_results.csv\", index=False)\n",
    "print(\"results saved to random_forest_results.csv successfully!\")"
   ],
   "id": "8ae043304098e04f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final summary - random forest on Breast Cancer Dataset\n",
      "============================================================\n",
      "Partition: 20/80\n",
      "  Avg Train Accuracy: 0.9971\n",
      "  Avg CV Accuracy: 0.9383\n",
      "  Avg Test Accuracy: 0.9452\n",
      "\n",
      "Partition: 50/50\n",
      "  Avg Train Accuracy: 0.9977\n",
      "  Avg CV Accuracy: 0.9601\n",
      "  Avg Test Accuracy: 0.9591\n",
      "\n",
      "Partition: 80/20\n",
      "  Avg Train Accuracy: 0.9985\n",
      "  Avg CV Accuracy: 0.9648\n",
      "  Avg Test Accuracy: 0.9561\n",
      "\n",
      "results saved to random_forest_results.csv successfully!\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
