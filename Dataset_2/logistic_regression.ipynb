{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T07:08:01.305067Z",
     "start_time": "2025-12-02T07:08:01.195229Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"breast_cancer_data.csv\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T07:08:03.853710Z",
     "start_time": "2025-12-02T07:08:02.895708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df.drop(\"id\", axis = 1, inplace = True) #uneccessary for classification.\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({'M': 1, 'B' : 0}) #one hot encoding the target variable.\n",
    "\n",
    "#splitting features columns from the target variable column.\n",
    "X = df.drop(\"diagnosis\", axis =1)\n",
    "Y = df[\"diagnosis\"]\n",
    "\n",
    "#making a transformer that scales all numerical values.\n",
    "numerical_cols = X.columns.tolist()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [(\"num\", StandardScaler(), numerical_cols)])\n",
    "\n",
    "partitions = [\n",
    "    (0.2, 0.8, \"20/80\"),\n",
    "    (0.5, 0.5, \"50/50\"),\n",
    "    (0.8, 0.2, \"80/20\"),\n",
    "]"
   ],
   "id": "b32dec77a860172f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T07:08:37.668594Z",
     "start_time": "2025-12-02T07:08:35.404921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#code training logistic regression starts here\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "results=[]\n",
    "for training_size, testing_size, partition_name in partitions:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Partition: {partition_name} (Train/Test)\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "\n",
    "    current_partition_results = []\n",
    "    for trial in range(3):\n",
    "        print(f\"Trial: {trial+1}\")\n",
    "\n",
    "        #splitting training and testing data and building a pipeline for processing data and classification.\n",
    "        X_training, X_testing, Y_training, Y_testing = train_test_split(X, Y, stratify = Y, test_size=testing_size, random_state=42+trial)\n",
    "        logistic_reg_pipeline = Pipeline([(\"preprocessor\", preprocessor),\n",
    "                                          (\"classifier\", LogisticRegression())])\n",
    "\n",
    "        parameter_grid = {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "            'classifier__penalty': ['l1', 'l2'], #type of regularization\n",
    "            'classifier__solver': ['liblinear', 'saga'] #optimization algo\n",
    "        }\n",
    "\n",
    "        #tuning hyperparameters\n",
    "        grid_search = GridSearchCV(logistic_reg_pipeline, parameter_grid, n_jobs = -1, cv = 5, verbose = 0,scoring=\"accuracy\")\n",
    "        grid_search.fit(X_training, Y_training)\n",
    "        optimum_model = grid_search.best_estimator_\n",
    "\n",
    "        #getting predictions from the best model with optimum hyperparameters.\n",
    "        training_prediction = optimum_model.predict(X_training)\n",
    "        testing_prediction = optimum_model.predict(X_testing)\n",
    "\n",
    "        #calculating accuracies for training, cross validation and testing.\n",
    "        training_accuracy = accuracy_score(Y_training, training_prediction)\n",
    "        testing_accuracy = accuracy_score(Y_testing, testing_prediction)\n",
    "        print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "        print(f\"Training Accuracy: {training_accuracy:.4f}\")\n",
    "        print(f\"Testing Accuracy: {testing_accuracy:.4f}\")\n",
    "        print(f\"Cross validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "        trial_results = {\n",
    "            \"Partition name\": partition_name,\n",
    "            \"Trial number\": trial+1,\n",
    "            \"Training accuracy\": training_accuracy,\n",
    "            \"Testing accuracy\": testing_accuracy,\n",
    "            \"Cross validation accuracy\":grid_search.best_score_,\n",
    "            \"Best parameters\":grid_search.best_params_\n",
    "        }\n",
    "        current_partition_results.append(trial_results)\n",
    "\n",
    "    #calculating average accuracies\n",
    "    avg_training_accuracy = np.mean([r[\"Training accuracy\"] for r in current_partition_results])\n",
    "    avg_testing_accuracy = np.mean([r[\"Testing accuracy\"] for r in current_partition_results])\n",
    "    avg_cross_validation_accuracy = np.mean([r[\"Cross validation accuracy\"] for r in current_partition_results])\n",
    "\n",
    "    avg_accuracy_summary = {\n",
    "        \"Partition name\": partition_name,\n",
    "        \"Partition results\": current_partition_results,\n",
    "        \"Average training accuracy\": avg_training_accuracy,\n",
    "        \"Average testing accuracy\": avg_testing_accuracy,\n",
    "        \"Average cross validation accuracy\": avg_cross_validation_accuracy,\n",
    "    }\n",
    "    results.append(avg_accuracy_summary)"
   ],
   "id": "20ebab2bbb80e0cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Partition: 20/80 (Train/Test)\n",
      "\n",
      "==================================================\n",
      "Trial: 1\n",
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Training Accuracy: 0.9735\n",
      "Testing Accuracy: 0.9737\n",
      "Cross validation accuracy: 0.9652\n",
      "Trial: 2\n",
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.9561\n",
      "Cross validation accuracy: 1.0000\n",
      "Trial: 3\n",
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Training Accuracy: 0.9735\n",
      "Testing Accuracy: 0.9671\n",
      "Cross validation accuracy: 0.9553\n",
      "\n",
      "==================================================\n",
      "Partition: 50/50 (Train/Test)\n",
      "\n",
      "==================================================\n",
      "Trial: 1\n",
      "Best Hyperparameters: {'classifier__C': 100, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Training Accuracy: 0.9894\n",
      "Testing Accuracy: 0.9754\n",
      "Cross validation accuracy: 0.9788\n",
      "Trial: 2\n",
      "Best Hyperparameters: {'classifier__C': 1, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Training Accuracy: 0.9930\n",
      "Testing Accuracy: 0.9754\n",
      "Cross validation accuracy: 0.9789\n",
      "Trial: 3\n",
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Training Accuracy: 0.9859\n",
      "Testing Accuracy: 0.9789\n",
      "Cross validation accuracy: 0.9648\n",
      "\n",
      "==================================================\n",
      "Partition: 80/20 (Train/Test)\n",
      "\n",
      "==================================================\n",
      "Trial: 1\n",
      "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Training Accuracy: 0.9824\n",
      "Testing Accuracy: 0.9825\n",
      "Cross validation accuracy: 0.9758\n",
      "Trial: 2\n",
      "Best Hyperparameters: {'classifier__C': 1, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Training Accuracy: 0.9868\n",
      "Testing Accuracy: 0.9912\n",
      "Cross validation accuracy: 0.9758\n",
      "Trial: 3\n",
      "Best Hyperparameters: {'classifier__C': 1, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Training Accuracy: 0.9934\n",
      "Testing Accuracy: 0.9561\n",
      "Cross validation accuracy: 0.9868\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T07:08:41.689186Z",
     "start_time": "2025-12-02T07:08:41.678451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#printing final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final summary - logistic regression on Breast Cancer Dataset\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Partition: {result['Partition name']}\")\n",
    "    print(f\"  Avg Train Accuracy: {result[\"Average training accuracy\"]:.4f}\")\n",
    "    print(f\"  Avg CV Accuracy: {result['Average cross validation accuracy']:.4f}\")\n",
    "    print(f\"  Avg Test Accuracy: {result['Average testing accuracy']:.4f}\")\n",
    "    print()\n",
    "\n",
    "compiled_results = []\n",
    "for result in results:\n",
    "    for partition in result[\"Partition results\"]:\n",
    "        compiled_results.append(partition)\n",
    "\n",
    "log_reg_results_csv = pd.DataFrame(compiled_results)\n",
    "log_reg_results_csv.to_csv(\"log_reg_results.csv\", index=False)\n",
    "print(\"results saved to xgboost_results.csv successfully!\")"
   ],
   "id": "c78e672d64fd69d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final summary - logistic regression on Breast Cancer Dataset\n",
      "============================================================\n",
      "Partition: 20/80\n",
      "  Avg Train Accuracy: 0.9823\n",
      "  Avg CV Accuracy: 0.9735\n",
      "  Avg Test Accuracy: 0.9656\n",
      "\n",
      "Partition: 50/50\n",
      "  Avg Train Accuracy: 0.9894\n",
      "  Avg CV Accuracy: 0.9742\n",
      "  Avg Test Accuracy: 0.9766\n",
      "\n",
      "Partition: 80/20\n",
      "  Avg Train Accuracy: 0.9875\n",
      "  Avg CV Accuracy: 0.9795\n",
      "  Avg Test Accuracy: 0.9766\n",
      "\n",
      "results saved to xgboost_results.csv successfully!\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
